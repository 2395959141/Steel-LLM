{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test qwen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 16:12:06,108] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n",
      "/home/calfa100/gqs/Steel-LLM/pretrain_modify_from_TinyLlama/model\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'qwen_15_1_8B_chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteel_modify_from_qwen_1_5\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqwen_15_1_8B_chat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_qwen2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Qwen2ForCausalLM\n\u001b[1;32m     12\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./steel_modify_from_qwen_1_5\u001b[39m\u001b[38;5;124m\"\u001b[39m,trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m config\u001b[38;5;241m.\u001b[39muse_cuda_rmsnorm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'qwen_15_1_8B_chat'"
     ]
    }
   ],
   "source": [
    "# 增加norm和rope的一致性\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy  as np\n",
    "from deepspeed.profiling.flops_profiler import get_model_profile  # type: ignore\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "sys.path.append(os.path.join(current_dir, \"steel_modify_from_qwen_1_5\"))\n",
    "from transformers import AutoConfig\n",
    "from steel_modify_from_qwen_1_5.modeling_steel import Qwen2ForCausalLM\n",
    "config = AutoConfig.from_pretrained(\"./steel_modify_from_qwen_1_5\",trust_remote_code=True)\n",
    "config.use_cuda_rmsnorm = True\n",
    "model = Qwen2ForCausalLM(config)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "memory_allocated = torch.cuda.memory_allocated(device)\n",
    "print(\"模型在显存中的占用大小：\", memory_allocated, \"字节\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"参数名称: {name}，数据类型: {param.dtype}\")\n",
    "    break\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 02:10:56,400] [INFO] [profiler.py:1218:get_model_profile] Flops profiler warming-up...\n",
      "[2024-05-10 02:10:56,420] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per GPU:                                                         1.84 B  \n",
      "params of model = params per GPU * mp_size:                             0       \n",
      "fwd MACs per GPU:                                                       422.72 GMACs\n",
      "fwd flops per GPU:                                                      845.47 G\n",
      "fwd flops of model = fwd flops per GPU * mp_size:                       845.47 G\n",
      "fwd latency:                                                            90.17 ms\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    9.38 TFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'Qwen2ForCausalLM': '1.84 B'}\n",
      "    MACs        - {'Qwen2ForCausalLM': '422.72 GMACs'}\n",
      "    fwd latency - {'Qwen2ForCausalLM': '90.17 ms'}\n",
      "depth 1:\n",
      "    params      - {'Qwen2Model': '1.53 B'}\n",
      "    MACs        - {'Qwen2Model': '343.06 GMACs'}\n",
      "    fwd latency - {'Qwen2Model': '81.17 ms'}\n",
      "depth 2:\n",
      "    params      - {'ModuleList': '1.21 B'}\n",
      "    MACs        - {'ModuleList': '343.06 GMACs'}\n",
      "    fwd latency - {'ModuleList': '80.3 ms'}\n",
      "depth 3:\n",
      "    params      - {'Qwen2DecoderLayer': '1.21 B'}\n",
      "    MACs        - {'Qwen2DecoderLayer': '343.06 GMACs'}\n",
      "    fwd latency - {'Qwen2DecoderLayer': '80.3 ms'}\n",
      "depth 4:\n",
      "    params      - {'Qwen2MLP': '811.6 M'}\n",
      "    MACs        - {'Qwen2MLP': '207.77 GMACs'}\n",
      "    fwd latency - {'Qwen2SdpaAttention': '37.19 ms'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "Qwen2ForCausalLM(\n",
      "  1.84 B = 100% Params, 422.72 GMACs = 100% MACs, 90.17 ms = 100% latency, 9.38 TFLOPS\n",
      "  (model): Qwen2Model(\n",
      "    1.53 B = 83.06% Params, 343.06 GMACs = 81.16% MACs, 81.17 ms = 90.02% latency, 8.45 TFLOPS\n",
      "    (embed_tokens): Embedding(311.16 M = 16.94% Params, 0 MACs = 0% MACs, 76.53 us = 0.08% latency, 0 FLOPS, 151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.43 ms = 3.8% latency, 8.34 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.57 ms = 1.75% latency, 7.16 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 297.55 us = 0.33% latency, 7.22 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 283.72 us = 0.31% latency, 7.57 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 280.62 us = 0.31% latency, 7.65 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 216.01 us = 0.24% latency, 9.94 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 40.77 us = 0.05% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.51 ms = 1.67% latency, 11.49 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 441.07 us = 0.49% latency, 13.08 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 435.35 us = 0.48% latency, 13.26 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 451.33 us = 0.5% latency, 12.79 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 45.54 us = 0.05% latency, 30.94 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 100.85 us = 0.11% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 90.36 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (1): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.31 ms = 3.67% latency, 8.64 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.52 ms = 1.69% latency, 7.41 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 286.34 us = 0.32% latency, 7.5 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 280.86 us = 0.31% latency, 7.65 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 280.62 us = 0.31% latency, 7.65 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 214.1 us = 0.24% latency, 10.03 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 35.76 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.47 ms = 1.63% latency, 11.78 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 437.97 us = 0.49% latency, 13.18 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 434.4 us = 0.48% latency, 13.29 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 446.56 us = 0.5% latency, 12.92 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 41.96 us = 0.05% latency, 33.58 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.31 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 88.45 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (2): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.33 ms = 3.7% latency, 8.58 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.52 ms = 1.68% latency, 7.44 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 287.06 us = 0.32% latency, 7.48 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 279.66 us = 0.31% latency, 7.68 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 281.1 us = 0.31% latency, 7.64 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 214.34 us = 0.24% latency, 10.02 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.48 ms = 1.64% latency, 11.7 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 439.88 us = 0.49% latency, 13.12 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 434.64 us = 0.48% latency, 13.28 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 446.32 us = 0.49% latency, 12.93 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 45.3 us = 0.05% latency, 31.1 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 94.18 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 95.13 us = 0.11% latency, 0 FLOPS)\n",
      "      )\n",
      "      (3): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.36 ms = 3.72% latency, 8.51 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.53 ms = 1.7% latency, 7.36 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 291.82 us = 0.32% latency, 7.36 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 256.3 us = 0.28% latency, 8.38 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 293.49 us = 0.33% latency, 7.32 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 211.95 us = 0.24% latency, 10.13 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 40.29 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.5 ms = 1.66% latency, 11.58 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 449.42 us = 0.5% latency, 12.84 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 434.88 us = 0.48% latency, 13.27 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 447.51 us = 0.5% latency, 12.9 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 45.06 us = 0.05% latency, 31.27 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.78 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 90.6 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (4): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.34 ms = 3.7% latency, 8.57 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.53 ms = 1.7% latency, 7.36 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 285.86 us = 0.32% latency, 7.51 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 281.33 us = 0.31% latency, 7.63 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 278.71 us = 0.31% latency, 7.71 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 218.63 us = 0.24% latency, 9.82 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.95 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.48 ms = 1.64% latency, 11.68 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 439.41 us = 0.49% latency, 13.13 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 435.11 us = 0.48% latency, 13.26 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 448.23 us = 0.5% latency, 12.88 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 44.11 us = 0.05% latency, 31.95 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.78 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 88.45 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (5): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.34 ms = 3.71% latency, 8.56 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.53 ms = 1.7% latency, 7.36 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 286.82 us = 0.32% latency, 7.49 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 284.19 us = 0.32% latency, 7.56 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 278.95 us = 0.31% latency, 7.7 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 212.67 us = 0.24% latency, 10.1 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 37.19 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.49 ms = 1.65% latency, 11.64 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 440.12 us = 0.49% latency, 13.11 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 437.26 us = 0.48% latency, 13.2 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 447.51 us = 0.5% latency, 12.9 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 46.25 us = 0.05% latency, 30.46 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.55 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 88.93 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (6): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.34 ms = 3.71% latency, 8.56 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.53 ms = 1.7% latency, 7.36 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 292.06 us = 0.32% latency, 7.35 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 281.57 us = 0.31% latency, 7.63 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 278.95 us = 0.31% latency, 7.7 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 213.62 us = 0.24% latency, 10.05 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.48 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.49 ms = 1.65% latency, 11.64 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 439.88 us = 0.49% latency, 13.12 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 435.35 us = 0.48% latency, 13.26 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 448.94 us = 0.5% latency, 12.86 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 44.82 us = 0.05% latency, 31.44 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.12 us = 0.09% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 87.74 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (7): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.35 ms = 3.72% latency, 8.53 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.53 ms = 1.7% latency, 7.35 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 290.39 us = 0.32% latency, 7.4 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 283.96 us = 0.31% latency, 7.56 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 279.9 us = 0.31% latency, 7.67 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 214.82 us = 0.24% latency, 10 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.24 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.49 ms = 1.66% latency, 11.6 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 439.41 us = 0.49% latency, 13.13 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 435.11 us = 0.48% latency, 13.26 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 447.27 us = 0.5% latency, 12.9 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 44.11 us = 0.05% latency, 31.95 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.83 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 87.5 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (8): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.35 ms = 3.71% latency, 8.54 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.53 ms = 1.69% latency, 7.38 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 288.96 us = 0.32% latency, 7.43 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 283 us = 0.31% latency, 7.59 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 260.59 us = 0.29% latency, 8.24 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 214.82 us = 0.24% latency, 10 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 39.82 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.49 ms = 1.65% latency, 11.66 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 439.64 us = 0.49% latency, 13.13 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 435.83 us = 0.48% latency, 13.24 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 447.03 us = 0.5% latency, 12.91 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.15 us = 0.05% latency, 32.65 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.07 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 99.18 us = 0.11% latency, 0 FLOPS)\n",
      "      )\n",
      "      (9): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.34 ms = 3.71% latency, 8.56 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.54 ms = 1.7% latency, 7.34 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 290.39 us = 0.32% latency, 7.4 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 281.33 us = 0.31% latency, 7.63 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 284.19 us = 0.32% latency, 7.56 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 212.43 us = 0.24% latency, 10.11 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.24 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.48 ms = 1.64% latency, 11.68 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 439.88 us = 0.49% latency, 13.12 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 435.83 us = 0.48% latency, 13.24 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 446.56 us = 0.5% latency, 12.92 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.63 us = 0.05% latency, 32.29 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.59 us = 0.09% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 87.98 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (10): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.35 ms = 3.72% latency, 8.53 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.54 ms = 1.71% latency, 7.3 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 291.35 us = 0.32% latency, 7.37 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 280.14 us = 0.31% latency, 7.67 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 282.05 us = 0.31% latency, 7.61 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 212.43 us = 0.24% latency, 10.11 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.95 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.48 ms = 1.64% latency, 11.71 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 439.64 us = 0.49% latency, 13.13 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 435.11 us = 0.48% latency, 13.26 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 446.08 us = 0.49% latency, 12.94 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.63 us = 0.05% latency, 32.29 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 84.64 us = 0.09% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 88.93 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (11): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.35 ms = 3.72% latency, 8.53 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.53 ms = 1.7% latency, 7.38 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 287.06 us = 0.32% latency, 7.48 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 281.81 us = 0.31% latency, 7.62 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 278.71 us = 0.31% latency, 7.71 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 213.62 us = 0.24% latency, 10.05 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 37.91 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.49 ms = 1.66% latency, 11.6 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 447.27 us = 0.5% latency, 12.9 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 435.59 us = 0.48% latency, 13.25 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 447.51 us = 0.5% latency, 12.9 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.63 us = 0.05% latency, 32.29 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.35 us = 0.09% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 87.26 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (12): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.35 ms = 3.71% latency, 8.53 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.53 ms = 1.7% latency, 7.35 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 288.01 us = 0.32% latency, 7.46 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 283 us = 0.31% latency, 7.59 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 281.1 us = 0.31% latency, 7.64 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 213.62 us = 0.24% latency, 10.05 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 37.91 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.49 ms = 1.65% latency, 11.62 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 439.17 us = 0.49% latency, 13.14 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 435.83 us = 0.48% latency, 13.24 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 448.47 us = 0.5% latency, 12.87 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.87 us = 0.05% latency, 32.12 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.31 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 89.65 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (13): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.35 ms = 3.71% latency, 8.54 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.54 ms = 1.7% latency, 7.34 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 289.2 us = 0.32% latency, 7.43 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 287.06 us = 0.32% latency, 7.48 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 279.43 us = 0.31% latency, 7.69 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 213.62 us = 0.24% latency, 10.05 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 37.91 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.48 ms = 1.64% latency, 11.68 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 438.93 us = 0.49% latency, 13.15 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 435.83 us = 0.48% latency, 13.24 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 445.6 us = 0.49% latency, 12.95 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 45.06 us = 0.05% latency, 31.27 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 87.74 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 89.41 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (14): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.35 ms = 3.72% latency, 8.53 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.54 ms = 1.71% latency, 7.32 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 289.68 us = 0.32% latency, 7.41 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 284.67 us = 0.32% latency, 7.54 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 280.38 us = 0.31% latency, 7.66 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 213.38 us = 0.24% latency, 10.06 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 37.19 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.49 ms = 1.65% latency, 11.66 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 440.6 us = 0.49% latency, 13.1 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 435.11 us = 0.48% latency, 13.26 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 447.03 us = 0.5% latency, 12.91 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.39 us = 0.05% latency, 32.47 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 87.02 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 89.65 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (15): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.35 ms = 3.72% latency, 8.53 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.54 ms = 1.7% latency, 7.34 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 285.39 us = 0.32% latency, 7.52 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 283.24 us = 0.31% latency, 7.58 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 283.72 us = 0.31% latency, 7.57 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 213.62 us = 0.24% latency, 10.05 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.95 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.49 ms = 1.65% latency, 11.62 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 438.45 us = 0.49% latency, 13.16 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 435.59 us = 0.48% latency, 13.25 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 445.37 us = 0.49% latency, 12.96 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 44.82 us = 0.05% latency, 31.44 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 87.02 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 87.5 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (16): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.84 ms = 4.26% latency, 7.44 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 2.04 ms = 2.26% latency, 5.52 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 295.88 us = 0.33% latency, 7.26 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 760.79 us = 0.84% latency, 2.82 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 288.25 us = 0.32% latency, 7.45 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 214.82 us = 0.24% latency, 10 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 37.19 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.48 ms = 1.64% latency, 11.69 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 439.88 us = 0.49% latency, 13.12 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 434.88 us = 0.48% latency, 13.27 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 446.08 us = 0.49% latency, 12.94 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 44.35 us = 0.05% latency, 31.77 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 84.4 us = 0.09% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 88.69 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (17): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.35 ms = 3.71% latency, 8.54 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.55 ms = 1.71% latency, 7.29 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 291.59 us = 0.32% latency, 7.36 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 283.24 us = 0.31% latency, 7.58 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 279.43 us = 0.31% latency, 7.69 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 212.67 us = 0.24% latency, 10.1 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 41.01 us = 0.05% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.48 ms = 1.64% latency, 11.7 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 438.93 us = 0.49% latency, 13.15 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 435.35 us = 0.48% latency, 13.26 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 447.75 us = 0.5% latency, 12.89 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.39 us = 0.05% latency, 32.47 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.83 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 88.93 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (18): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.36 ms = 3.73% latency, 8.5 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.53 ms = 1.69% latency, 7.39 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 287.06 us = 0.32% latency, 7.48 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 281.81 us = 0.31% latency, 7.62 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 284.43 us = 0.32% latency, 7.55 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 212.67 us = 0.24% latency, 10.1 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.48 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.49 ms = 1.65% latency, 11.65 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 440.12 us = 0.49% latency, 13.11 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 436.31 us = 0.48% latency, 13.23 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 447.27 us = 0.5% latency, 12.9 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.63 us = 0.05% latency, 32.29 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 88.21 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 110.86 us = 0.12% latency, 0 FLOPS)\n",
      "      )\n",
      "      (19): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.35 ms = 3.71% latency, 8.54 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.53 ms = 1.7% latency, 7.37 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 288.01 us = 0.32% latency, 7.46 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 282.76 us = 0.31% latency, 7.59 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 279.66 us = 0.31% latency, 7.68 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 215.29 us = 0.24% latency, 9.97 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.24 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.49 ms = 1.66% latency, 11.59 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 438.21 us = 0.49% latency, 13.17 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 437.97 us = 0.49% latency, 13.18 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 447.51 us = 0.5% latency, 12.9 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.06% latency, 27.11 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.83 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 89.41 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (20): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.34 ms = 3.7% latency, 8.56 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.53 ms = 1.7% latency, 7.36 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 288.49 us = 0.32% latency, 7.44 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 284.67 us = 0.32% latency, 7.54 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 277.52 us = 0.31% latency, 7.74 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 213.86 us = 0.24% latency, 10.04 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 37.19 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.48 ms = 1.65% latency, 11.67 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 439.41 us = 0.49% latency, 13.13 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 434.88 us = 0.48% latency, 13.27 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 447.75 us = 0.5% latency, 12.89 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.87 us = 0.05% latency, 32.12 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.83 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 88.69 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (21): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.34 ms = 3.7% latency, 8.56 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.53 ms = 1.69% latency, 7.39 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 287.29 us = 0.32% latency, 7.47 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 281.33 us = 0.31% latency, 7.63 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 283.48 us = 0.31% latency, 7.58 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 213.86 us = 0.24% latency, 10.04 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 35.29 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.49 ms = 1.65% latency, 11.63 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 439.41 us = 0.49% latency, 13.13 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 436.07 us = 0.48% latency, 13.24 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 450.85 us = 0.5% latency, 12.8 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.63 us = 0.05% latency, 32.29 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 87.02 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 88.45 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (22): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 3.14 ms = 3.49% latency, 9.09 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.55 ms = 1.72% latency, 7.28 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 291.59 us = 0.32% latency, 7.36 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 293.02 us = 0.32% latency, 7.33 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 280.38 us = 0.31% latency, 7.66 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 212.67 us = 0.24% latency, 10.1 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.72 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.27 ms = 1.41% latency, 13.62 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 368.36 us = 0.41% latency, 15.67 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 365.26 us = 0.41% latency, 15.8 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 374.79 us = 0.42% latency, 15.4 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.39 us = 0.05% latency, 32.47 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.55 us = 0.1% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 88.93 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "      (23): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.29 GMACs = 3.38% MACs, 2.99 ms = 3.32% latency, 9.56 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.64 GMACs = 1.33% MACs, 1.39 ms = 1.54% latency, 8.11 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 251.05 us = 0.28% latency, 8.55 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 240.8 us = 0.27% latency, 8.92 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 242.23 us = 0.27% latency, 8.87 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 184.77 us = 0.2% latency, 11.62 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 37.67 us = 0.04% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.05% MACs, 1.28 ms = 1.42% latency, 13.57 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 369.31 us = 0.41% latency, 15.63 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 370.98 us = 0.41% latency, 15.56 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.68% MACs, 375.03 us = 0.42% latency, 15.39 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.39 us = 0.05% latency, 32.47 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.35 us = 0.09% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 88.45 us = 0.1% latency, 0 FLOPS)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 87.5 us = 0.1% latency, 0 FLOPS)\n",
      "  )\n",
      "  (lm_head): Linear(311.16 M = 16.94% Params, 79.66 GMACs = 18.84% MACs, 8.45 ms = 9.37% latency, 18.85 TFLOPS, in_features=2048, out_features=151936, bias=False)\n",
      ")\n",
      "------------------------------------------------------------------------------\n",
      "[2024-05-10 02:10:56,580] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n",
      "FLOPs: 845.47 G\n",
      "MACs: 422.72 GMACs\n",
      "Params: 1.84 B\n",
      "tensor([[[-3.1961,  0.5018,  0.0545,  ..., -0.5740,  0.8570,  1.4311],\n",
      "         [-1.9279,  0.3841,  0.1334,  ...,  0.5241,  0.9317,  0.8956],\n",
      "         [-3.1839,  0.4064, -0.3141,  ...,  0.2762, -0.0115,  1.4722],\n",
      "         ...,\n",
      "         [-1.1093,  0.9614, -0.9871,  ..., -0.0385, -0.8155,  0.8621],\n",
      "         [-1.4960,  0.2056,  0.8101,  ...,  0.7258,  1.8004, -0.1097],\n",
      "         [-1.9091,  0.9909, -0.2968,  ...,  0.6407, -0.5000,  2.1551]]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "********************\n",
      "[2024-05-10 02:10:56,644] [INFO] [profiler.py:1218:get_model_profile] Flops profiler warming-up...\n",
      "[2024-05-10 02:10:56,663] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per GPU:                                                         1.84 B  \n",
      "params of model = params per GPU * mp_size:                             0       \n",
      "fwd MACs per GPU:                                                       429.16 GMACs\n",
      "fwd flops per GPU:                                                      858.36 G\n",
      "fwd flops of model = fwd flops per GPU * mp_size:                       858.36 G\n",
      "fwd latency:                                                            37.33 ms\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    22.99 TFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'Qwen2ForCausalLM': '1.84 B'}\n",
      "    MACs        - {'Qwen2ForCausalLM': '429.16 GMACs'}\n",
      "    fwd latency - {'Qwen2ForCausalLM': '37.33 ms'}\n",
      "depth 1:\n",
      "    params      - {'Qwen2Model': '1.53 B'}\n",
      "    MACs        - {'Qwen2Model': '349.5 GMACs'}\n",
      "    fwd latency - {'Qwen2Model': '35.99 ms'}\n",
      "depth 2:\n",
      "    params      - {'ModuleList': '1.21 B'}\n",
      "    MACs        - {'ModuleList': '349.5 GMACs'}\n",
      "    fwd latency - {'ModuleList': '34.74 ms'}\n",
      "depth 3:\n",
      "    params      - {'Qwen2DecoderLayer': '1.21 B'}\n",
      "    MACs        - {'Qwen2DecoderLayer': '349.5 GMACs'}\n",
      "    fwd latency - {'Qwen2DecoderLayer': '34.74 ms'}\n",
      "depth 4:\n",
      "    params      - {'Qwen2MLP': '811.6 M'}\n",
      "    MACs        - {'Qwen2MLP': '207.77 GMACs'}\n",
      "    fwd latency - {'Qwen2SdpaAttention': '17.95 ms'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "Qwen2ForCausalLM(\n",
      "  1.84 B = 100% Params, 429.16 GMACs = 100% MACs, 37.33 ms = 100% latency, 22.99 TFLOPS\n",
      "  (model): Qwen2Model(\n",
      "    1.53 B = 83.06% Params, 349.5 GMACs = 81.44% MACs, 35.99 ms = 96.42% latency, 19.42 TFLOPS\n",
      "    (embed_tokens): Embedding(311.16 M = 16.94% Params, 0 MACs = 0% MACs, 78.2 us = 0.21% latency, 0 FLOPS, 151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.57 ms = 4.21% latency, 18.54 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 830.41 us = 2.22% latency, 14.22 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 99.18 us = 0.27% latency, 21.65 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 76.29 us = 0.2% latency, 28.15 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 72.48 us = 0.19% latency, 29.63 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 77.72 us = 0.21% latency, 27.63 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 40.05 us = 0.11% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 388.62 us = 1.04% latency, 44.56 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.25 us = 0.21% latency, 74.71 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 72.48 us = 0.19% latency, 79.63 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 78.92 us = 0.21% latency, 73.13 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 45.78 us = 0.12% latency, 30.78 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 105.86 us = 0.28% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 89.17 us = 0.24% latency, 0 FLOPS)\n",
      "      )\n",
      "      (1): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.43 ms = 3.84% latency, 20.31 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 746.25 us = 2% latency, 15.83 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 83.45 us = 0.22% latency, 25.73 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 68.43 us = 0.18% latency, 31.38 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 67 us = 0.18% latency, 32.05 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 72.96 us = 0.2% latency, 29.44 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 374.32 us = 1% latency, 46.26 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 75.1 us = 0.2% latency, 76.85 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 71.76 us = 0.19% latency, 80.42 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.72 us = 0.21% latency, 74.25 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 42.2 us = 0.11% latency, 33.39 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.07 us = 0.23% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 87.02 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (2): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.44 ms = 3.86% latency, 20.23 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 741.96 us = 1.99% latency, 15.92 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 80.59 us = 0.22% latency, 26.65 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 67.47 us = 0.18% latency, 31.83 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 67.71 us = 0.18% latency, 31.72 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 71.76 us = 0.19% latency, 29.92 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 35.52 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 375.03 us = 1% latency, 46.17 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 75.34 us = 0.2% latency, 76.6 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 71.76 us = 0.19% latency, 80.42 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.72 us = 0.21% latency, 74.25 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 41.72 us = 0.11% latency, 33.77 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 96.56 us = 0.26% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.59 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (3): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.48 ms = 3.95% latency, 19.74 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 785.11 us = 2.1% latency, 15.04 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 80.11 us = 0.21% latency, 26.81 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 67.71 us = 0.18% latency, 31.72 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 66.76 us = 0.18% latency, 32.17 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 74.15 us = 0.2% latency, 28.96 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 35.29 us = 0.09% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 378.37 us = 1.01% latency, 45.76 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 78.92 us = 0.21% latency, 73.13 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 72.24 us = 0.19% latency, 79.89 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.72 us = 0.21% latency, 74.25 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 42.2 us = 0.11% latency, 33.39 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 83.45 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 87.5 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (4): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.43 ms = 3.84% latency, 20.31 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 730.51 us = 1.96% latency, 16.17 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 79.39 us = 0.21% latency, 27.05 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 68.66 us = 0.18% latency, 31.27 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 64.61 us = 0.17% latency, 33.24 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 73.19 us = 0.2% latency, 29.34 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 34.09 us = 0.09% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 393.15 us = 1.05% latency, 44.04 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 76.06 us = 0.2% latency, 75.88 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 73.19 us = 0.2% latency, 78.85 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.01 us = 0.21% latency, 74.94 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 44.35 us = 0.12% latency, 31.77 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 83.45 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.55 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (5): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.41 ms = 3.79% latency, 20.61 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 730.75 us = 1.96% latency, 16.16 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 78.44 us = 0.21% latency, 27.38 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 68.66 us = 0.18% latency, 31.27 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 65.33 us = 0.17% latency, 32.87 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 73.19 us = 0.2% latency, 29.34 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 35.76 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 373.6 us = 1% latency, 46.35 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 74.63 us = 0.2% latency, 77.34 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 71.53 us = 0.19% latency, 80.69 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.01 us = 0.21% latency, 74.94 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 42.2 us = 0.11% latency, 33.39 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 81.78 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.83 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (6): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.42 ms = 3.81% latency, 20.48 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 735.04 us = 1.97% latency, 16.07 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 77.96 us = 0.21% latency, 27.54 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 68.66 us = 0.18% latency, 31.27 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 65.57 us = 0.18% latency, 32.75 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 73.67 us = 0.2% latency, 29.15 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.48 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 379.09 us = 1.02% latency, 45.68 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 75.1 us = 0.2% latency, 76.85 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 72.48 us = 0.19% latency, 79.63 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.96 us = 0.21% latency, 74.03 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.15 us = 0.12% latency, 32.65 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 83.45 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.83 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (7): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.44 ms = 3.87% latency, 20.17 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 747.92 us = 2% latency, 15.79 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 78.68 us = 0.21% latency, 27.29 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 67.71 us = 0.18% latency, 31.72 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 66.52 us = 0.18% latency, 32.28 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 85.12 us = 0.23% latency, 25.23 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 35.76 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 383.14 us = 1.03% latency, 45.19 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 75.1 us = 0.2% latency, 76.85 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 73.43 us = 0.2% latency, 78.59 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.49 us = 0.21% latency, 74.48 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.15 us = 0.12% latency, 32.65 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 82.73 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.35 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (8): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.44 ms = 3.86% latency, 20.2 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 743.15 us = 1.99% latency, 15.89 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 81.06 us = 0.22% latency, 26.49 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 69.62 us = 0.19% latency, 30.85 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 67.47 us = 0.18% latency, 31.83 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 72 us = 0.19% latency, 29.83 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.48 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 385.05 us = 1.03% latency, 44.97 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 75.82 us = 0.2% latency, 76.12 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 72 us = 0.19% latency, 80.16 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.96 us = 0.21% latency, 74.03 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.63 us = 0.12% latency, 32.29 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 82.02 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.31 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (9): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.43 ms = 3.83% latency, 20.35 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 737.67 us = 1.98% latency, 16.01 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 78.68 us = 0.21% latency, 27.29 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 68.66 us = 0.18% latency, 31.27 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 67 us = 0.18% latency, 32.05 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 72 us = 0.19% latency, 29.83 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.72 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 380.04 us = 1.02% latency, 45.56 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 74.86 us = 0.2% latency, 77.09 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 71.76 us = 0.19% latency, 80.42 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 76.77 us = 0.21% latency, 75.18 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.15 us = 0.12% latency, 32.65 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 83.45 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.07 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (10): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.46 ms = 3.9% latency, 19.99 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 756.5 us = 2.03% latency, 15.61 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 79.15 us = 0.21% latency, 27.13 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 67.95 us = 0.18% latency, 31.6 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 78.68 us = 0.21% latency, 27.29 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 72.48 us = 0.19% latency, 29.63 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.24 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 381.47 us = 1.02% latency, 45.39 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 75.34 us = 0.2% latency, 76.6 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 71.29 us = 0.19% latency, 80.96 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 78.2 us = 0.21% latency, 73.8 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.87 us = 0.12% latency, 32.12 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 84.16 us = 0.23% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 87.5 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (11): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.44 ms = 3.84% latency, 20.29 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 736.47 us = 1.97% latency, 16.04 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 78.68 us = 0.21% latency, 27.29 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 68.19 us = 0.18% latency, 31.49 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 66.28 us = 0.18% latency, 32.4 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 71.53 us = 0.19% latency, 30.02 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.48 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 383.85 us = 1.03% latency, 45.11 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 74.86 us = 0.2% latency, 77.09 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 72.48 us = 0.19% latency, 79.63 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.96 us = 0.21% latency, 74.03 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.87 us = 0.12% latency, 32.12 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 82.97 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.83 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (12): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.44 ms = 3.85% latency, 20.27 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 739.57 us = 1.98% latency, 15.97 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 80.11 us = 0.21% latency, 26.81 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 68.9 us = 0.18% latency, 31.17 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 66.52 us = 0.18% latency, 32.28 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 72.72 us = 0.19% latency, 29.53 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.48 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 382.66 us = 1.03% latency, 45.25 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 74.86 us = 0.2% latency, 77.09 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 72.96 us = 0.2% latency, 79.11 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 76.77 us = 0.21% latency, 75.18 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.15 us = 0.12% latency, 32.65 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 82.97 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.55 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (13): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.48 ms = 3.97% latency, 19.63 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 752.45 us = 2.02% latency, 15.7 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 83.68 us = 0.22% latency, 25.66 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 72 us = 0.19% latency, 29.83 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 69.14 us = 0.19% latency, 31.06 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 72 us = 0.19% latency, 29.83 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.95 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 388.15 us = 1.04% latency, 44.61 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 75.82 us = 0.2% latency, 76.12 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 72 us = 0.19% latency, 80.16 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.01 us = 0.21% latency, 74.94 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 45.78 us = 0.12% latency, 30.78 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.12 us = 0.23% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 96.08 us = 0.26% latency, 0 FLOPS)\n",
      "      )\n",
      "      (14): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.44 ms = 3.84% latency, 20.29 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 737.43 us = 1.98% latency, 16.02 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 79.63 us = 0.21% latency, 26.97 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 69.38 us = 0.19% latency, 30.95 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 67.23 us = 0.18% latency, 31.94 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 70.57 us = 0.19% latency, 30.43 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 35.76 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 382.42 us = 1.02% latency, 45.28 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.01 us = 0.21% latency, 74.94 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 71.29 us = 0.19% latency, 80.96 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.49 us = 0.21% latency, 74.48 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.15 us = 0.12% latency, 32.65 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 83.45 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.12 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (15): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.45 ms = 3.88% latency, 20.12 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 744.34 us = 1.99% latency, 15.87 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 81.78 us = 0.22% latency, 26.26 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 68.66 us = 0.18% latency, 31.27 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 67.47 us = 0.18% latency, 31.83 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 72.24 us = 0.19% latency, 29.73 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.24 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 388.15 us = 1.04% latency, 44.61 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 83.68 us = 0.22% latency, 68.97 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 71.53 us = 0.19% latency, 80.69 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.01 us = 0.21% latency, 74.94 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 42.68 us = 0.11% latency, 33.02 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 83.45 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.78 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (16): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.43 ms = 3.84% latency, 20.31 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 740.29 us = 1.98% latency, 15.95 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 81.3 us = 0.22% latency, 26.41 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 67.47 us = 0.18% latency, 31.83 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 67.23 us = 0.18% latency, 31.94 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 71.29 us = 0.19% latency, 30.12 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.48 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 377.89 us = 1.01% latency, 45.82 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 74.39 us = 0.2% latency, 77.59 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 72 us = 0.19% latency, 80.16 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 76.77 us = 0.21% latency, 75.18 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 42.68 us = 0.11% latency, 33.02 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 84.4 us = 0.23% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.35 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (17): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.44 ms = 3.85% latency, 20.28 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 743.39 us = 1.99% latency, 15.89 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 81.78 us = 0.22% latency, 26.26 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 68.19 us = 0.18% latency, 31.49 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 66.52 us = 0.18% latency, 32.28 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 72.72 us = 0.19% latency, 29.53 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 37.67 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 376.94 us = 1.01% latency, 45.94 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 74.15 us = 0.2% latency, 77.84 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 70.81 us = 0.19% latency, 81.5 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.01 us = 0.21% latency, 74.94 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 41.96 us = 0.11% latency, 33.58 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 83.45 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 86.31 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (18): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.44 ms = 3.86% latency, 20.2 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 746.73 us = 2% latency, 15.82 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 79.39 us = 0.21% latency, 27.05 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 68.9 us = 0.18% latency, 31.17 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 65.33 us = 0.17% latency, 32.87 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 72.24 us = 0.19% latency, 29.73 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.48 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 383.62 us = 1.03% latency, 45.14 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 75.82 us = 0.2% latency, 76.12 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 71.53 us = 0.19% latency, 80.69 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.72 us = 0.21% latency, 74.25 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.87 us = 0.12% latency, 32.12 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 81.3 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 85.12 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (19): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.43 ms = 3.83% latency, 20.39 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 735.76 us = 1.97% latency, 16.05 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 78.68 us = 0.21% latency, 27.29 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 69.62 us = 0.19% latency, 30.85 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 65.57 us = 0.18% latency, 32.75 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 72.24 us = 0.19% latency, 29.73 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 36.48 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 381.71 us = 1.02% latency, 45.36 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 75.34 us = 0.2% latency, 76.6 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 71.29 us = 0.19% latency, 80.96 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.01 us = 0.21% latency, 74.94 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 42.92 us = 0.11% latency, 32.83 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 81.3 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 84.64 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (20): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.43 ms = 3.84% latency, 20.33 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 735.76 us = 1.97% latency, 16.05 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 78.92 us = 0.21% latency, 27.21 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 69.14 us = 0.19% latency, 31.06 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 65.8 us = 0.18% latency, 32.63 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 71.76 us = 0.19% latency, 29.92 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 37.19 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 383.62 us = 1.03% latency, 45.14 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 74.39 us = 0.2% latency, 77.59 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 72.48 us = 0.19% latency, 79.63 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.49 us = 0.21% latency, 74.48 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.39 us = 0.12% latency, 32.47 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 81.78 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 84.4 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (21): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.45 ms = 3.87% latency, 20.14 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 748.63 us = 2.01% latency, 15.78 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 79.63 us = 0.21% latency, 26.97 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 77.96 us = 0.21% latency, 27.54 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 70.81 us = 0.19% latency, 30.33 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 70.33 us = 0.19% latency, 30.53 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 35.52 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 384.09 us = 1.03% latency, 45.08 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 74.86 us = 0.2% latency, 77.09 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 72 us = 0.19% latency, 80.16 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.72 us = 0.21% latency, 74.25 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 43.15 us = 0.12% latency, 32.65 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 82.73 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 84.88 us = 0.23% latency, 0 FLOPS)\n",
      "      )\n",
      "      (22): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.43 ms = 3.83% latency, 20.36 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 739.81 us = 1.98% latency, 15.97 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 79.15 us = 0.21% latency, 27.13 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 70.57 us = 0.19% latency, 30.43 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 66.28 us = 0.18% latency, 32.4 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 70.81 us = 0.19% latency, 30.33 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 35.76 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 380.75 us = 1.02% latency, 45.48 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 74.86 us = 0.2% latency, 77.09 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 71.76 us = 0.19% latency, 80.42 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.72 us = 0.21% latency, 74.25 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 42.44 us = 0.11% latency, 33.2 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 81.78 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 83.92 us = 0.22% latency, 0 FLOPS)\n",
      "      )\n",
      "      (23): Qwen2DecoderLayer(\n",
      "        50.6 M = 2.75% Params, 14.56 GMACs = 3.39% MACs, 1.49 ms = 3.99% latency, 19.54 TFLOPS\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          16.78 M = 0.91% Params, 5.91 GMACs = 1.38% MACs, 762.46 us = 2.04% latency, 15.49 TFLOPS\n",
      "          (q_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 79.39 us = 0.21% latency, 27.05 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 67.47 us = 0.18% latency, 31.83 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(4.2 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 65.09 us = 0.17% latency, 32.99 TFLOPS, in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(4.19 M = 0.23% Params, 1.07 GMACs = 0.25% MACs, 85.83 us = 0.23% latency, 25.02 TFLOPS, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 35.52 us = 0.1% latency, 0 FLOPS)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          33.82 M = 1.84% Params, 8.66 GMACs = 2.02% MACs, 393.63 us = 1.05% latency, 43.99 TFLOPS\n",
      "          (gate_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 77.72 us = 0.21% latency, 74.25 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 71.05 us = 0.19% latency, 81.23 TFLOPS, in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(11.27 M = 0.61% Params, 2.89 GMACs = 0.67% MACs, 87.26 us = 0.23% latency, 66.14 TFLOPS, in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 42.92 us = 0.11% latency, 32.83 GFLOPS)\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 81.78 us = 0.22% latency, 0 FLOPS)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 100.37 us = 0.27% latency, 0 FLOPS)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 82.97 us = 0.22% latency, 0 FLOPS)\n",
      "  )\n",
      "  (lm_head): Linear(311.16 M = 16.94% Params, 79.66 GMACs = 18.56% MACs, 653.74 us = 1.75% latency, 243.7 TFLOPS, in_features=2048, out_features=151936, bias=False)\n",
      ")\n",
      "------------------------------------------------------------------------------\n",
      "[2024-05-10 02:10:56,738] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n",
      "FLOPs: 858.36 G\n",
      "MACs: 429.16 GMACs\n",
      "Params: 1.84 B\n",
      "tensor([[[-3.2031,  0.5039,  0.0244,  ..., -0.5859,  0.8633,  1.4219],\n",
      "         [-1.9219,  0.3730,  0.1226,  ...,  0.5156,  0.9492,  0.8789],\n",
      "         [-3.1562,  0.4043, -0.3008,  ...,  0.2969, -0.0122,  1.4766],\n",
      "         ...,\n",
      "         [-1.1016,  0.9648, -0.9648,  ..., -0.0232, -0.8438,  0.8711],\n",
      "         [-1.4766,  0.2002,  0.8008,  ...,  0.6836,  1.7578, -0.1416],\n",
      "         [-1.8438,  0.9883, -0.2695,  ...,  0.6523, -0.4922,  2.1719]]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 666\n",
    "torch.manual_seed(seed)  # 设置随机种子\n",
    "torch.cuda.manual_seed(seed)  # 设置CUDA的随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # 如果使用多个GPU，设置所有GPU的随机种子\n",
    "np.random.seed(seed)\n",
    "fake_input = torch.randint(1, 10000, (1, 256),dtype=torch.long, device=model.device)\n",
    "input_dict = {\"input_ids\": fake_input, \"labels\": fake_input.clone()}\n",
    "model = model.to(torch.float32)\n",
    "flops, macs, params = get_model_profile(model, kwargs=input_dict, print_profile=True, detailed=True)\n",
    "print(\"FLOPs:\", flops)\n",
    "print(\"MACs:\", macs)\n",
    "print(\"Params:\", params)\n",
    "output = model(fake_input)\n",
    "print(output.logits[:,:,:10])\n",
    "print(\"*\"*20)\n",
    "#===========================================================\n",
    "model = model.to(torch.bfloat16)\n",
    "flops, macs, params = get_model_profile(model, kwargs=input_dict, print_profile=True, detailed=True)\n",
    "print(\"FLOPs:\", flops)\n",
    "print(\"MACs:\", macs)\n",
    "print(\"Params:\", params)\n",
    "output = model(fake_input)\n",
    "print(output.logits[:,:,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试 rms+rope一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn.layers.rotary import apply_rotary_emb_func as __apply_rotary_emb_func\n",
    "from flash_attn.ops.rms_norm import rms_norm as __rms_norm\n",
    "from torch import nn\n",
    "import torch\n",
    "class Qwen2RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Qwen2RMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[ 0.3965, -0.6992, -1.8359, -0.8242, -1.3672,  1.4688,  0.1836,  1.0703,\n",
      "         -1.0859, -1.9062, -0.2969, -0.3633, -0.9648, -0.0815,  0.1748, -0.3105]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.3965, -0.6992, -1.8359, -0.8242, -1.3672,  1.4688,  0.1836,  1.0703,\n",
      "         -1.0859, -1.9062, -0.2969, -0.3633, -0.9648, -0.0815,  0.1748, -0.3105]],\n",
      "       device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<DropoutAddLayerNormFnBackward>)\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6\n",
    "dim = 16\n",
    "input  = torch.randn([1,dim],device=\"cuda:0\",dtype=torch.bfloat16)\n",
    "torch_rmsnorm = Qwen2RMSNorm(dim, eps=eps).to(\"cuda:0\")\n",
    "print(torch_rmsnorm.weight.dtype)\n",
    "output1 = torch_rmsnorm(input)\n",
    "#=\n",
    "output2 = __rms_norm(input, torch_rmsnorm.weight, torch_rmsnorm.variance_epsilon)\n",
    "print(output1)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test recurrentgemma\n",
    "from transformers import AutoConfig\n",
    "from recurrentgemma.modeling_recurrent_gemma import RecurrentGemmaForCausalLM\n",
    "config = AutoConfig.from_pretrained(\"./recurrentgemma\",trust_remote_code=True)\n",
    "model = RecurrentGemmaForCausalLM(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
